{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7A4CCcsZWmZ9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, concatenate,multiply, LayerNormalization, Add\n",
    "from keras.layers import Lambda\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2WwKwkzWmaH"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return inputs + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUBos8kFWmaJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.5\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xjxnSvaWmaK"
   },
   "source": [
    "## Utility functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKtiyaymWmaL"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_compiled_model():\n",
    "    dense_dim = 1\n",
    "    num_heads = 1\n",
    "    classes = 14\n",
    "\n",
    "    #Middle Input LSTM\n",
    "    rgb_input_1 = Input(shape=(1024), name='input_rgb_1')\n",
    "    rgb_output_1 = Dense(128, activation='relu', name='output_rgb_1')(rgb_input_1)\n",
    "\n",
    "    #Middle Input LSTM\n",
    "    rgb_input_2 = Input(shape=(1024), name='input_rgb_2')\n",
    "    rgb_output_2 = Dense(128, activation='relu')(rgb_input_2)\n",
    "\n",
    "    #Right Input LSTM\n",
    "    rgb_input_3 = Input(shape=(1024), name='input_rgb_3')\n",
    "    rgb_output_3 = Dense(128, activation='relu')(rgb_input_3)\n",
    "    \n",
    "    \n",
    "    merged_rgb = concatenate([rgb_output_1, rgb_output_2, rgb_output_3], name='RGB_Concatenate')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    inputs_l = keras.Input(shape=(None, None))\n",
    "    x1 = PositionalEmbedding(64, 58, name=\"frame_position_embedding1\")(inputs_l)\n",
    "    x1= TransformerEncoder(58, dense_dim, num_heads, name=\"transformer_layer1\")(x1)\n",
    "    x1 = layers.GlobalMaxPooling1D()(x1)\n",
    "    x1 = layers.Dropout(0.4)(x1)\n",
    "    x1 = layers.Dense(64)(x1)\n",
    "    \n",
    "    inputs_m = keras.Input(shape=(None, None))\n",
    "    x2 = PositionalEmbedding(63, 29, name=\"frame_position_embedding2\")(inputs_m)\n",
    "    x2 = TransformerEncoder(29, dense_dim, num_heads, name=\"transformer_layer2\")(x2)\n",
    "    x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "    x2 = layers.Dropout(0.4)(x2)\n",
    "    x2 = layers.Dense(64)(x2)\n",
    "    \n",
    "    \n",
    "    inputs_r = keras.Input(shape=(None, None))\n",
    "    x3 = PositionalEmbedding(64, 27, name=\"frame_position_embedding3\")(inputs_r)\n",
    "    x3 = TransformerEncoder(27, dense_dim, num_heads, name=\"transformer_layer3\")(x3)\n",
    "    x3 = layers.GlobalMaxPooling1D()(x3)\n",
    "    x3 = layers.Dropout(0.4)(x3)\n",
    "    x3 = layers.Dense(64)(x3)\n",
    "    \n",
    "    mer = concatenate([x1, x2, x3], name='Concatenate')\n",
    "    mer = layers.Dropout(0.4)(mer)\n",
    "    \n",
    "    final_merge = concatenate([mer,merged_rgb])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     x = layers.Dense(64, activation=\"relu\")(x)\n",
    "#     x = layers.Dense(128, activation=\"tanh\")(x)\n",
    "    outputs = layers.Dense(14, activation=\"sigmoid\")(final_merge)\n",
    "#     model = keras.Model(inputs, outputs)\n",
    "    model = keras.Model(inputs=[inputs_l, inputs_m,inputs_r, rgb_input_1, rgb_input_2, rgb_input_3], outputs=outputs, name='Final_output')\n",
    "\n",
    "    model.compile(optimizer=\"SGD\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jRa-CtN0bM0c",
    "outputId": "c15a3ecc-64ea-4478-dbfb-98046ccb7132"
   },
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(pd.read_csv(\"train_samples_updated.csv\"))[:, 5:]\n",
    "val_labels = np.array(pd.read_csv(\"val_samples_updated.csv\"))[:, 5:]\n",
    "# labels.shape\n",
    "rgb_1_train = np.load(\"path_to_final_swin_rgb_train_view1.npy\") #update file path\n",
    "rgb_0_train = np.load(\"path_to_final_swin_rgb_train_view0.npy\")  #update file path\n",
    "rgb_2_train = np.load(\"path_to_final_swin_rgb_train_view2.npy\")  #update file path\n",
    "print(rgb_1_train.shape, rgb_0_train.shape, rgb_2_train.shape)\n",
    "\n",
    "rgb_1_val = np.load(\"path_to_final_swin_rgb_val_view1.npy\")  #update file path\n",
    "rgb_0_val = np.load(\"path_to_final_swin_rgb_val_view0.npy\")  #update file path\n",
    "rgb_2_val = np.load(\"path_to_final_swin_rgb_val_view2.npy\")  #update file path\n",
    "print(rgb_1_val.shape, rgb_0_val.shape, rgb_2_val.shape)\n",
    "\n",
    "rgb_1_test = np.load(\"path_to_final_swin_rgb_test_view1.npy\")  #update file path\n",
    "rgb_0_test = np.load(\"path_to_final_swin_rgb_test_view0.npy\")  #update file path\n",
    "rgb_2_test = np.load(\"path_to_final_swin_rgb_test_view2.npy\")  #update file path\n",
    "print(rgb_1_test.shape, rgb_0_test.shape, rgb_2_test.shape)  #update file path\n",
    "\n",
    "dct_1_train = np.load(\"path_to_swin_mul_dct_train_view1.npy\")  #update file path\n",
    "dct_0_train = np.load(\"swin_mul_dct_train_view0.npy\")  #update file path\n",
    "dct_2_train = np.load(\"swin_mul_dct_train_view2.npy\")  #update file path\n",
    "print(dct_1_train.shape, dct_0_train.shape, dct_2_train.shape)\n",
    "\n",
    "dct_1_val = np.load(\"path_to_swin_mul_dct_val_view1.npy\")  #update file path\n",
    "dct_0_val = np.load(\"path_to_swin_mul_dct_val_view0.npy\")  #update file path\n",
    "dct_2_val = np.load(\"path_to_swin_mul_dct_val_view2.npy\")  #update file path\n",
    "print(dct_1_val.shape, dct_0_val.shape, dct_2_val.shape)\n",
    "\n",
    "dct_1_test = np.load(\"path_to_swin_mul_dct_test_view1.npy\")  #update file path\n",
    "dct_0_test = np.load(\"path_to_swin_mul_dct_test_view0.npy\")  #update file path\n",
    "dct_2_test = np.load(\"path_to_swin_mul_dct_test_view2.npy\")  #update file path\n",
    "print(dct_1_test.shape, dct_0_test.shape, dct_2_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_C = np.load(\"path_to_train_C.npy\")  #update file path\n",
    "val_C = np.load(\"path_to_val_C.npy\")  #update file path\n",
    "test_C = np.load(\"path_to_test_C.npy\")  #update file path\n",
    "sc = StandardScaler()\n",
    "\n",
    "s0, s1, s2 = train_C.shape[0], train_C.shape[1], train_C.shape[2]\n",
    "# print(s0,s1,s2)\n",
    "train_C = train_C.reshape(s0 *s1, s2)\n",
    "sc.fit(train_C)\n",
    "\n",
    "train_C = sc.transform(train_C)\n",
    "train_C = train_C.reshape(s0, s1, s2)\n",
    "\n",
    "s0, s1, s2 = val_C.shape[0], val_C.shape[1], val_C.shape[2]\n",
    "val_C = val_C.reshape(s0 * s1, s2)\n",
    "val_C = sc.transform(val_C)\n",
    "val_C = val_C.reshape(s0, s1, s2)\n",
    "\n",
    "s0, s1, s2 = test_C.shape[0], test_C.shape[1], test_C.shape[2]\n",
    "test_C = test_C.reshape(s0 * s1, s2)\n",
    "test_C = sc.transform(test_C)\n",
    "test_C = test_C.reshape(s0, s1, s2)\n",
    "\n",
    "\n",
    "train_T = np.load(\"path_to_train_T.npy\")  #update file path\n",
    "val_T = np.load(\"path_to_val_T.npy\")  #update file path\n",
    "test_T = np.load(\"path_to_test_T.npy\")  #update file path\n",
    "sc = StandardScaler()\n",
    "\n",
    "s0, s1, s2 = train_T.shape[0], train_T.shape[1], train_T.shape[2]\n",
    "train_T = train_T.reshape(s0 * s1, s2)\n",
    "sc.fit(train_T)\n",
    "train_T = sc.transform(train_T)\n",
    "train_T = train_T.reshape(s0, s1, s2)\n",
    "\n",
    "s0, s1, s2 = val_T.shape[0], val_T.shape[1], val_T.shape[2]\n",
    "val_T = val_T.reshape(s0 * s1, s2)\n",
    "val_T = sc.transform(val_T)\n",
    "val_T = val_T.reshape(s0, s1, s2)\n",
    "\n",
    "s0, s1, s2 =test_T.shape[0], test_T.shape[1], test_T.shape[2]\n",
    "test_T = test_T.reshape(s0 * s1, s2)\n",
    "test_T = sc.transform(test_T)\n",
    "test_T = test_T.reshape(s0, s1, s2)\n",
    "\n",
    "\n",
    "\n",
    "train_S = np.load(\"path_to_train_S.npy\")  #update file path\n",
    "val_S = np.load(\"path_to_val_S.npy\")  #update file path\n",
    "test_S = np.load(\"path_to_test_S.npy\")  #update file path\n",
    "sc = StandardScaler()\n",
    "\n",
    "s0, s1, s2 = train_S.shape[0], train_S.shape[1], train_S.shape[2]\n",
    "train_S = train_S.reshape(s0 * s1, s2)\n",
    "sc.fit(train_S)\n",
    "train_S = sc.transform(train_S)\n",
    "train_S = train_S.reshape(s0, s1, s2)\n",
    "\n",
    "s0, s1, s2 = val_S.shape[0], val_S.shape[1], val_S.shape[2]\n",
    "val_S = val_S.reshape(s0 * s1, s2)\n",
    "val_S = sc.transform(val_S)\n",
    "val_S = val_S.reshape(s0, s1, s2)\n",
    "\n",
    "s0, s1, s2 = test_S.shape[0], test_S.shape[1], test_S.shape[2]\n",
    "test_S = test_S.reshape(s0 * s1, s2)\n",
    "test_S = sc.transform(test_S)\n",
    "test_S = test_S.reshape(s0, s1, s2)\n",
    "\n",
    "print(train_C.shape, val_C.shape, test_C.shape, train_T.shape, val_T.shape, test_T.shape, train_S.shape, val_S.shape, test_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pht05zpVbBka"
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "filepath = \"/tmp/video_classifier\"\n",
    "history = model.fit([train_C, train_T, train_S, rgb_1_train, rgb_0_train, rgb_2_train], train_labels, batch_size = 64, epochs=50, validation_data=([val_C, val_T, val_S, rgb_1_val, rgb_0_val, rgb_2_val], val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict([val_C, val_T, val_S, rgb_1_val, rgb_0_val, rgb_2_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_file = pd.read_csv(\"path_val_samples_updated.csv\") #Update path\n",
    "\n",
    "col_names = ['rec_no', 'subject_pos', 'start_time', 'end_time' ]\n",
    "new_train_csv = train_csv_file.drop(col_names, axis=1)\n",
    "\n",
    "#preparing train and test csv\n",
    "test_csv = new_train_csv\n",
    "# train_csv = new_train_csv[3123:]\n",
    "print(len(test_csv))\n",
    "\n",
    "Column_names = ['Settle','Legs crossed','Groom','Hand-mouth','Fold arms','Leg movement','Scratch','Gesture','Hand-face','Adjusting clothing','Fumble','Shrug','Stretching','Smearing hands']\n",
    "# Column_names = ['Hand-face','Hand-mouth','Gesture','Fumble','Scratch','Stretching','Smearing hands','Shrug','Adjusting clothing','Groom','Fold arms','Leg movement','Settle','Legs crossed']\n",
    "extracted_col = test_csv[\"sample_id\"]\n",
    "test_pred_csv = pd.DataFrame(predict, columns = Column_names)\n",
    "test_pred_csv.insert(0, \"sample_id\", extracted_col)\n",
    "test_pred_csv.to_csv(\"test_predicted_transformer\"  + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For epochs: {0}**************************************************************\".format(epochs))\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "CLASSES = ['Hand-face','Hand-mouth','Gesture','Fumble','Scratch','Stretching','Smearing hands','Shrug','Adjusting clothing','Groom','Fold arms','Leg movement','Settle','Legs crossed']\n",
    "\n",
    "\n",
    "def evaluate(test_annotation_file,user_submission_file):\n",
    "#     test = pd.read_csv(test_annotation_file,index_col=\"sample_id\").sort_values('sample_id')\n",
    "#     user = pd.read_csv(user_submission_file,index_col=\"sample_id\").sort_values('sample_id')\n",
    "    \n",
    "    test = test_annotation_file.sort_values('sample_id')\n",
    "    \n",
    "    user = user_submission_file.sort_values('sample_id')\n",
    "    if not(np.all(test.index==user.index)):\n",
    "        raise ValueError(\"Indexes of test and prediction files do not agree.\")\n",
    "        \n",
    "    scores = []\n",
    "    for behaviour in CLASSES:\n",
    "        cur_score = average_precision_score(test[behaviour].values,user[behaviour].values)\n",
    "        scores.append(cur_score)\n",
    "    per_class_scores = pd.DataFrame({'behaviour':CLASSES,'score':scores}).set_index('behaviour')\n",
    "    macro_average = np.mean(scores)\n",
    "    return {'macro_average':macro_average,'per_class_scores':per_class_scores}\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # example usage of evaluate function\n",
    "    test_annotation_file = test_csv\n",
    "    user_submission_file = test_pred_csv # use your own predictions here\n",
    "    results = evaluate(test_annotation_file,user_submission_file)\n",
    "    print('')\n",
    "    print('--------------- MACRO AVERAGE: -----------------')\n",
    "    print('')\n",
    "    print(str(results['macro_average']))\n",
    "    print('')\n",
    "    print('--------------- PER CLASS: ---------------------')\n",
    "    print(str(results['per_class_scores']))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
